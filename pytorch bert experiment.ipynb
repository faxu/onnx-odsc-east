{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference PyTorch Bert Model for High Performance in ONNX Runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to load a BERT model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime with transformer optimization. In the following sections, we are going to use the BERT model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. BERT SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial Roadmap\n",
    "\n",
    "- Install a few necessary packages (PyTorch, Transformers, TorchVision, wget).\n",
    "- Load a pretrained BERT SQuAD model from a source PyTorch implementation.\n",
    "- Export our PyTorch BERT model to ONNX.\n",
    "- Compare inference on our models for PyTorch and ONNX Runtime.\n",
    "- Optimize our inference with execution providers using the ONNX Go Live (OLive) tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "First you need to check if the following packages exist and install them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (3.2)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch==1.3.1\n",
      "  Using cached https://files.pythonhosted.org/packages/65/96/c97c8a0ea8f66de41f452925b521bcfdebef6fffb899dc704fc269d87563/torch-1.3.1-cp36-none-macosx_10_7_x86_64.whl\n",
      "Collecting torchvision==0.4.2\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/8c/53a88b9a18d8edb33019519f9595bfd5add2ff5aeba19e7402b950906edf/torchvision-0.4.2-cp36-cp36m-macosx_10_7_x86_64.whl\n",
      "Requirement already satisfied: numpy in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from torch==1.3.1) (1.18.2)\n",
      "Requirement already satisfied: six in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from torchvision==0.4.2) (1.14.0)\n",
      "Collecting pillow>=4.1.1 (from torchvision==0.4.2)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/3a/1cb999d3f9311f9b7c6387b81ec7b5373d50ef031b957994898e59697c18/Pillow-7.0.0-cp36-cp36m-macosx_10_6_intel.whl\n",
      "Installing collected packages: torch, pillow, torchvision\n",
      "Successfully installed pillow-7.0.0 torch-1.3.1 torchvision-0.4.2\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers==2.5.1 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (2.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (2020.2.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (4.44.1)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (1.12.31)\n",
      "Requirement already satisfied: sacremoses in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (0.0.38)\n",
      "Requirement already satisfied: filelock in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (0.1.85)\n",
      "Requirement already satisfied: numpy in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (1.18.2)\n",
      "Requirement already satisfied: requests in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from transformers==2.5.1) (2.23.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from boto3->transformers==2.5.1) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from boto3->transformers==2.5.1) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from boto3->transformers==2.5.1) (1.15.31)\n",
      "Requirement already satisfied: six in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from sacremoses->transformers==2.5.1) (1.14.0)\n",
      "Requirement already satisfied: click in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from sacremoses->transformers==2.5.1) (7.1.1)\n",
      "Requirement already satisfied: joblib in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from sacremoses->transformers==2.5.1) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from requests->transformers==2.5.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from requests->transformers==2.5.1) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from requests->transformers==2.5.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from requests->transformers==2.5.1) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.5.1) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers==2.5.1) (0.15.2)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: psutil in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (5.7.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "!pip install wget\n",
    "!pip install torch==1.3.1\n",
    "!pip install torchvision==0.4.2\n",
    "!pip install transformers==2.5.1\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained BERT model\n",
    "We begin by downloading the data files and store them in the specified pytorch_output and pytorch_squad directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading predict file.\n",
      "Predict file downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory to store predict file\n",
    "output_dir = \"./pytorch_output\"\n",
    "cache_dir = \"./pytorch_squad\"\n",
    "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
    "# create cache dir\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "# Download the file\n",
    "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
    "if not os.path.exists(predict_file):\n",
    "    import wget\n",
    "    print(\"Start downloading predict file.\")\n",
    "    wget.download(predict_file_url, predict_file)\n",
    "    print(\"Predict file downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify some relevant model config / hyperparameter variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define some variables. As an example, we used batch size 1 and max sequence length 128. \n",
    "model_type = \"bert\"\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "max_seq_length = 128\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "eval_batch_size = 1\n",
    "# The hardware you'd like to use to run the model.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_convert_examples_to_features??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to load our BERT model into PyTorch from our pretrained files. This step could take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974397ef42384286a1fea9d09fa53667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fdd6755fa34507af90daacd719f8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0b1f20715f42fe9e04651cef47a1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.32it/s]\n",
      "convert squad examples to features: 100%|██████████| 3/3 [00:00<00:00, 90.65it/s]\n",
      "add example index and unique id: 100%|██████████| 3/3 [00:00<00:00, 3416.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# The following code is adapted from HuggingFace transformers\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py#L290\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "from torch.utils.data import (DataLoader, SequentialSampler)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=False,\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir)\n",
    "\n",
    "# Load and Convert the examples from the downloaded predict file into a list of features \n",
    "# that can be directly given as input to a model.\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_dev_examples(None, filename=predict_file)\n",
    "\n",
    "from transformers import squad_convert_examples_to_features\n",
    "features, dataset = squad_convert_examples_to_features( \n",
    "            examples=examples[:3], # convert only 3 examples for demo\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the loaded model\n",
    "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation {} *****\n",
      "  Num examples =  6\n",
      "  Batch size =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages/torch/onnx/symbolic_opset9.py:427: UserWarning: ONNX export squeeze with negative axis -1 might cause the onnx model to be incorrect. Negative axis is not supported in ONNX. Axis is converted to 2 based on input shape at export time. Passing an tensor of different rank in execution will be incorrect.\n",
      "  \"Passing an tensor of different rank in execution will be incorrect.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported at  ./pytorch_squad/bert-base-cased-squad.onnx\n"
     ]
    }
   ],
   "source": [
    "# Eval!\n",
    "print(\"***** Running evaluation {} *****\")\n",
    "print(\"  Num examples = \", len(dataset))\n",
    "print(\"  Batch size = \", eval_batch_size)\n",
    "\n",
    "# create output dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_model_path = './pytorch_squad/bert-base-cased-squad.onnx'    \n",
    "inputs = {}\n",
    "outputs= {}\n",
    "# Get the first batch of data to run the model and export it to ONNX\n",
    "batch = dataset[0]\n",
    "\n",
    "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
    "# inference and training mode.\n",
    "model.eval()\n",
    "batch = tuple(t.to(device) for t in batch)\n",
    "inputs = {\n",
    "    'input_ids':      batch[0].reshape(1, 128),                         # using batch size = 1 here. Adjust as needed.\n",
    "    'attention_mask': batch[1].reshape(1, 128),\n",
    "    'token_type_ids': batch[2].reshape(1, 128)\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "    torch.onnx.export(model,                                            # model being run\n",
    "                      (inputs['input_ids'],                             # model input (or a tuple for multiple inputs)\n",
    "                       inputs['attention_mask'], \n",
    "                       inputs['token_type_ids']), \n",
    "                      output_model_path,                                # where to save the model (can be a file or file-like object)\n",
    "                      opset_version=11,                                 # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
    "                      input_names=['input_ids',                         # the model's input names\n",
    "                                   'input_mask', \n",
    "                                   'segment_ids'],\n",
    "                      output_names=['start', 'end'],                    # the model's output names\n",
    "                      dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
    "                                    'input_mask' : symbolic_names,\n",
    "                                    'segment_ids' : symbolic_names,\n",
    "                                    'start' : symbolic_names,\n",
    "                                    'end' : symbolic_names})\n",
    "    print(\"Model exported at \", output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference the Exported Model with ONNX Runtime\n",
    "#### Install ONNX Runtime\n",
    "Install ONNX Runtime if you haven't done so already. Make sure to install the correct package from PyPi -- onnxruntime to use CPU features, or onnxruntime-gpu to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Using cached https://files.pythonhosted.org/packages/68/db/ce33eae1c701547b99d778b64a9694e16efa7353ab98dd60d20402bdabda/onnxruntime-1.2.0-cp36-cp36m-macosx_10_14_x86_64.whl\n",
      "Collecting onnx>=1.2.3 (from onnxruntime)\n",
      "  Using cached https://files.pythonhosted.org/packages/83/53/2a51e046fb94bb924556341d09f82f08bc5cc515cf4764ceb3feeebc763a/onnx-1.6.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from onnxruntime) (1.18.2)\n",
      "Collecting protobuf (from onnx>=1.2.3->onnxruntime)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/65/3ac73d6a9f31de4b45ebff6885e9a4ccd16eccb63764dd406140d337fabd/protobuf-3.11.3-cp36-cp36m-macosx_10_9_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from onnx>=1.2.3->onnxruntime) (1.14.0)\n",
      "Collecting typing-extensions>=3.6.2.1 (from onnx>=1.2.3->onnxruntime)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/92/705fe8aca27678e01bbdd7738173b8e7df0088a2202c80352f664630d638/typing_extensions-3.7.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /Users/prroy/Documents/MachineLearning/conferences/odsc_env/lib/python3.6/site-packages (from protobuf->onnx>=1.2.3->onnxruntime) (40.6.2)\n",
      "Installing collected packages: protobuf, typing-extensions, onnx, onnxruntime\n",
      "Successfully installed onnx-1.6.0 onnxruntime-1.2.0 protobuf-3.11.3 typing-extensions-3.7.4.1\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ONNXRUNTIME = 'onnxruntime'\n",
    "# Install ONNX Runtime\n",
    "if torch.cuda.is_available():\n",
    "    ## Install onnxruntime-gpu if cuda is available\n",
    "    ONNXRUNTIME = 'onnxruntime-gpu'\n",
    "!pip install $ONNXRUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to inference our model with ONNX Runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime inference time:  0.3330717086791992\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt  \n",
    "import time\n",
    "import psutil\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "\n",
    "# Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization. This is enabled on default.\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "# The following settings enables OpenMP, which is required to get best performance for CPU inference of Bert models.\n",
    "sess_options.intra_op_num_threads=1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "session = rt.InferenceSession(output_model_path, sess_options)\n",
    "\n",
    "# evaluate the model\n",
    "start = time.time()\n",
    "res = session.run(None, {\n",
    "          'input_ids': inputs['input_ids'].cpu().numpy(),\n",
    "          'input_mask': inputs['attention_mask'].cpu().numpy(),\n",
    "          'segment_ids': inputs['token_type_ids'].cpu().numpy()\n",
    "        })\n",
    "end = time.time()\n",
    "print(\"ONNX Runtime inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get comparative performance numbers from the original PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference time =  0.23375296592712402\n",
      "***** Verifying correctness *****\n",
      "PyTorch and ORT matching numbers: True\n",
      "PyTorch and ORT matching numbers: True\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "outputs = model(**inputs)\n",
    "end = time.time()\n",
    "print(\"PyTorch Inference time = \", end - start)\n",
    "\n",
    "print(\"***** Verifying correctness *****\")\n",
    "import numpy as np\n",
    "for i in range(2):\n",
    "    print('PyTorch and ORT matching numbers:', np.allclose(res[i], outputs[i].cpu().detach().numpy(), rtol=1e-04, atol=1e-05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
